{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk,os\n",
    "from bs4 import BeautifulSoup\n",
    "from sentita import calculate_polarity\n",
    "\n",
    "nlp = spacy.load('it_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(content,date,url):\n",
    "            soup = BeautifulSoup(content, \"lxml\")\n",
    "            title = soup.find(\"h1\",{\"class\": \"entry-title\"}).text.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "            \n",
    "            tags = \";\".join([clean(x.text) for x in soup.findAll(\"a\",{\"rel\": \"tag\"})])\n",
    "\n",
    "            area = \";\".join([clean(x.text) for x in soup.findAll(\"a\",{\"class\": \"categoria\"})])\n",
    "            \n",
    "            content = \" \".join([clean(x.text) for x in soup.findAll(\"p\")])\n",
    "\n",
    "            content = content.split(\"Dal 2010 gli articoli del Post sono sempre stati gratuiti e accessibili a tutti, e lo resteranno: perché ogni lettore in più è una persona che sa delle cose in più, e migliora il mondo.\")[0]\n",
    "\n",
    "            sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "            ent_dict = {\"ORG\":[],\"LOC\":[],\"PER\":[],\"THINGS\":[]}\n",
    "\n",
    "            for orig_sent in sentences:\n",
    "                \n",
    "                \n",
    "                orig_sent = orig_sent.replace(\"”\",\" \")\n",
    "                \n",
    "                #print (sent)\n",
    "                \n",
    "                #pos = round(calculate_polarity([sent])[0][0] - calculate_polarity([sent])[0][1],3)\n",
    "                sent = nlp(orig_sent)\n",
    "                \n",
    "                \n",
    "                #print (sent)\n",
    "                #print (pos)\n",
    "                \n",
    "                #annotated_article = tagme.annotate(str(sent),lang=\"it\")\n",
    "\n",
    "                #annot = {}\n",
    "\n",
    "                #check = {}\n",
    "\n",
    "                #for ann in annotated_article.get_annotations(0.2):\n",
    "                #    annot[ann.mention] = ann.entity_title\n",
    "\n",
    "\n",
    "\n",
    "                for ent in sent.ents:\n",
    "                    if ent.label_ in ent_dict:\n",
    "\n",
    "                  #      if ent.text in annot:\n",
    "                  #          dis_ent = annot[ent.text]\n",
    "                   #         ent_dict[ent.label_].append(dis_ent)\n",
    "                   #         check[ent.text] = \"\"\n",
    "\n",
    "                    #    else:\n",
    "                            ent_dict[ent.label_].append(ent.text.strip())\n",
    "                        \n",
    "                            if ent.text.strip() in store_sentences:\n",
    "                                if date in store_sentences[ent.text.strip()]:\n",
    "                                    if area in store_sentences[ent.text.strip()][date]:\n",
    "                                        store_sentences[ent.text.strip()][date][area].append([orig_sent,url])\n",
    "                                    else:\n",
    "                                        store_sentences[ent.text.strip()][date][area] = [[orig_sent,url]]\n",
    "                                else:\n",
    "                                    store_sentences[ent.text.strip()][date] = {area:[[orig_sent,url]]}\n",
    "                            else:\n",
    "                                store_sentences[ent.text.strip()] = {date:{area:[[orig_sent,url]]}}\n",
    "                        \n",
    "\n",
    "            #    for dis in annot.keys():\n",
    "            #        if dis not in check:\n",
    "             #           ent_dict[\"THINGS\"].append(annot[dis])\n",
    "\n",
    "                #for x,y in ent_dict.items():\n",
    "                #    print (x, len(y))\n",
    "                #print (\" \")\n",
    "            line = title+\"\\t\"+\";\".join(ent_dict[\"PER\"])+\"\\t\"+\";\".join(ent_dict[\"LOC\"])+\"\\t\"+\";\".join(ent_dict[\"ORG\"])+\"\\t\"+\";\".join(ent_dict[\"THINGS\"])+\"\\t\"+tags+\"\\t\"+area\n",
    "            return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 1 0\n",
      "2010 2 0\n",
      "2010 3 0\n",
      "2010 4 0\n",
      "2010 5 0\n",
      "2010 6 0\n",
      "2010 7 0\n",
      "2010 8 0\n",
      "2010 9 0\n",
      "2010 10 0\n",
      "2010 11 681\n",
      "2010 12 1328\n",
      "2011 1 1680\n",
      "2011 2 1680\n",
      "2011 3 1680\n",
      "2011 4 1680\n",
      "2011 5 1680\n",
      "2011 6 1680\n",
      "2011 7 1680\n",
      "2011 8 1680\n",
      "2011 9 1680\n",
      "2011 10 1680\n",
      "2011 11 2058\n",
      "2011 12 2504\n",
      "2012 1 2710\n",
      "2012 2 2710\n",
      "2012 3 2710\n",
      "2012 4 2710\n",
      "2012 5 2710\n",
      "2012 6 2710\n",
      "2012 7 2710\n",
      "2012 8 2710\n",
      "2012 9 2710\n",
      "2012 10 2710\n",
      "2012 11 3178\n",
      "2012 12 3592\n",
      "2013 1 3971\n",
      "2013 2 3971\n",
      "2013 3 3971\n",
      "2013 4 3971\n",
      "2013 5 3971\n",
      "2013 6 3971\n",
      "2013 7 3971\n",
      "2013 8 3971\n",
      "2013 9 3971\n",
      "2013 10 3971\n",
      "2013 11 4578\n",
      "2013 12 5115\n",
      "2014 1 5600\n",
      "2014 2 5600\n",
      "2014 3 5600\n",
      "2014 4 5600\n",
      "2014 5 5600\n",
      "2014 6 5600\n",
      "2014 7 5600\n",
      "2014 8 5600\n",
      "2014 9 5600\n",
      "2014 10 5600\n",
      "2014 11 6664\n",
      "2014 12 7505\n",
      "2015 1 8085\n",
      "2015 2 8085\n",
      "2015 3 8085\n",
      "2015 4 8085\n",
      "2015 5 8085\n",
      "2015 6 8085\n",
      "2015 7 8085\n",
      "2015 8 8085\n",
      "2015 9 8085\n",
      "2015 10 8085\n",
      "2015 11 9886\n",
      "2015 12 11281\n",
      "2016 1 12583\n",
      "2016 2 12583\n",
      "2016 3 12583\n",
      "2016 4 12583\n",
      "2016 5 12583\n",
      "2016 6 12583\n",
      "2016 7 12583\n",
      "2016 8 12583\n",
      "2016 9 12583\n",
      "2016 10 12583\n",
      "2016 11 14917\n",
      "2016 12 16699\n",
      "2017 1 18614\n",
      "2017 2 18614\n",
      "2017 3 18614\n",
      "2017 4 18614\n",
      "2017 5 18614\n",
      "2017 6 18614\n",
      "2017 7 18614\n",
      "2017 8 18614\n",
      "2017 9 18614\n",
      "2017 10 18614\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output = open(\"post-largerdump.tsv\",\"w\")\n",
    "\n",
    "store_sentences = {}\n",
    "\n",
    "output.write(\"url\\tdate\\ttitle\\tPER\\tLOC\\tORG\\tTHINGS\\tTOPICS\\tAREA\\n\")\n",
    "\n",
    "\n",
    "folder = os.listdir(\"www.ilpost.it/\")\n",
    "\n",
    "years = [str(x) for x in range(2010,2020)]\n",
    "\n",
    "months = [str(x) for x in range(1,13)]\n",
    "\n",
    "days = [str(x) for x in range(1,31)]\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        print (year,month, len(store_sentences))\n",
    "        for day in days:\n",
    "            try:\n",
    "                if day in os.listdir(\"www.ilpost.it/\"+year+\"/\"+month+\"/\"):\n",
    "                    for article in os.listdir(\"www.ilpost.it/\"+year+\"/\"+month+\"/\"+day+\"/\"):\n",
    "                        date = year+month+day\n",
    "                        url = \"www.ilpost.it/\"+year+\"/\"+month+\"/\"+day+\"/\"+article+\"/\"\n",
    "                        \n",
    "                        content = open(\"www.ilpost.it/\"+year+\"/\"+month+\"/\"+day+\"/\"+article+\"/index.html\",\"r\").read()\n",
    "                        line = url +\"\\t\"+date+\"\\t\"+parse_page(content,date,url)\n",
    "                        line = line.replace(\"\\n\",\" \")\n",
    "                        output.write(line+\"\\n\")\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "output.close()    \n",
    "\n",
    "import json\n",
    "\n",
    "with open('store_sentences.json', 'w') as fp:\n",
    "    json.dump(store_sentences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
